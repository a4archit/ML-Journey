{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db490e41-a060-4797-9575-1e495109eb06",
   "metadata": {},
   "source": [
    "# Tensors in Pytorch\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401e03b8-458d-47be-b435-f432b6a8bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd1d760-be20-456c-872c-1d032d3228eb",
   "metadata": {},
   "source": [
    "\n",
    "## Creation\n",
    "```\n",
    "x = torch.randn(*size)              # tensor with independent N(0,1) entries\n",
    "x = torch.[ones|zeros](*size)       # tensor with all 1's [or 0's]\n",
    "x = torch.tensor(L)                 # create tensor from [nested] list or ndarray L\n",
    "y = x.clone()                       # clone of x\n",
    "with torch.no_grad():               # code wrap that stops autograd from tracking tensor history\n",
    "requires_grad=True                  # arg, when set to True, tracks computation\n",
    "                                    # history for future derivative calculations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77f12594-9c22-4458-979b-e450d32cfcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45586575-5389-4b34-9117-f03bde467a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((3,3), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d07129d1-d936-42ea-a5dc-7c07fe9ab999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((3,3), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8180f3d-4db2-4ffa-be41-31a5f67c86b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[         0,          0, 1953702523],\n",
       "        [1702130273,  540680804,  842019362],\n",
       "        [ 909126965, 1412509997,  842674224]], dtype=torch.int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty((3,3), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6332db34-b66e-4529-ab2a-f673dd04ffe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5117, 0.0197, 0.3306],\n",
       "        [0.6186, 0.3496, 0.4891],\n",
       "        [0.9648, 0.6378, 0.5571]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand((3,3), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43612fc6-88bc-4672-a8b4-66acd15e9124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1117, 0.8158, 0.2626],\n",
       "        [0.4839, 0.6765, 0.7539],\n",
       "        [0.2627, 0.0428, 0.2080]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual seed \n",
    "torch.manual_seed(100)\n",
    "torch.rand((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34f7006e-9b22-4f76-95de-dbe219f21825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  6, 11, 16, 21],\n",
       "        [26, 31, 36, 41, 46]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1,50, 5).reshape(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8682be56-e32a-4d11-9837-e797372a4dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "        [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]], dtype=torch.int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1,20,20, dtype=torch.int32).reshape(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8fdbef2e-4b91-4504-a740-46c6baf78466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e44dcac-5304-48d7-bde8-95779cb7fbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[          762275552, 4607182418800017408, 4607182418800017408],\n",
       "        [                 97,           761983776,           763054672]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38f0d843-9d53-43f4-ad68-e223fec36176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e72789eb-1138-482c-b81d-7acc295a0818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a3f566be-71e6-4dbd-a1ec-7936d3d51ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1180, 0.1217, 0.7356],\n",
       "        [0.7118, 0.7876, 0.4183]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand_like(x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd07f24-8751-498d-b5fd-84ac6ff02dcf",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Types\n",
    "\n",
    "| **Data Type**             | **Dtype**         | **Description**                                                                                                                                                                |\n",
    "|---------------------------|-------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **32-bit Floating Point** | `torch.float32`   | Standard floating-point type used for most deep learning tasks. Provides a balance between precision and memory usage.                                                         |\n",
    "| **64-bit Floating Point** | `torch.float64`   | Double-precision floating point. Useful for high-precision numerical tasks but uses more memory.                                                                               |\n",
    "| **16-bit Floating Point** | `torch.float16`   | Half-precision floating point. Commonly used in mixed-precision training to reduce memory and computational overhead on modern GPUs.                                            |\n",
    "| **BFloat16**              | `torch.bfloat16`  | Brain floating-point format with reduced precision compared to `float16`. Used in mixed-precision training, especially on TPUs.                                                |\n",
    "| **8-bit Floating Point**  | `torch.float8`    | Ultra-low-precision floating point. Used for experimental applications and extreme memory-constrained environments (less common).                                               |\n",
    "| **8-bit Integer**         | `torch.int8`      | 8-bit signed integer. Used for quantized models to save memory and computation in inference.                                                                                   |\n",
    "| **16-bit Integer**        | `torch.int16`     | 16-bit signed integer. Useful for special numerical tasks requiring intermediate precision.                                                                                    |\n",
    "| **32-bit Integer**        | `torch.int32`     | Standard signed integer type. Commonly used for indexing and general-purpose numerical tasks.                                                                                  |\n",
    "| **64-bit Integer**        | `torch.int64`     | Long integer type. Often used for large indexing arrays or for tasks involving large numbers.                                                                                  |\n",
    "| **8-bit Unsigned Integer**| `torch.uint8`     | 8-bit unsigned integer. Commonly used for image data (e.g., pixel values between 0 and 255).                                                                                    |\n",
    "| **Boolean**               | `torch.bool`      | Boolean type, stores `True` or `False` values. Often used for masks in logical operations.                                                                                      |\n",
    "| **Complex 64**            | `torch.complex64` | Complex number type with 32-bit real and 32-bit imaginary parts. Used for scientific and signal processing tasks.                                                               |\n",
    "| **Complex 128**           | `torch.complex128`| Complex number type with 64-bit real and 64-bit imaginary parts. Offers higher precision but uses more memory.                                                                 |\n",
    "| **Quantized Integer**     | `torch.qint8`     | Quantized signed 8-bit integer. Used in quantized models for efficient inference.                                                                                              |\n",
    "| **Quantized Unsigned Integer** | `torch.quint8` | Quantized unsigned 8-bit integer. Often used for quantized tensors in image-related tasks.                                                                                     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e3092b-c1b2-4299-90a0-823f78b4f6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93635b18-4fce-40a2-816a-960452234a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef67a52-ad47-4ff9-a3ca-99095537b73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54622160-26e5-4763-a5b2-9e2b94d3e209",
   "metadata": {},
   "source": [
    "***\n",
    "## Dimensionality\n",
    "```\n",
    "x.size()                                  # return tuple-like object of dimensions\n",
    "x = torch.cat(tensor_seq, dim=0)          # concatenates tensors along dim\n",
    "y = x.view(a,b,...)                       # reshapes x into size (a,b,...)\n",
    "y = x.view(-1,a)                          # reshapes x into size (b,a) for some b\n",
    "y = x.transpose(a,b)                      # swaps dimensions a and b\n",
    "y = x.permute(*dims)                      # permutes dimensions\n",
    "y = x.unsqueeze(dim)                      # tensor with added axis\n",
    "y = x.unsqueeze(dim=2)                    # (a,b,c) tensor -> (a,b,1,c) tensor\n",
    "y = x.squeeze()                           # removes all dimensions of size 1 (a,1,b,1) -> (a,b)\n",
    "y = x.squeeze(dim=1)                      # removes specified dimension of size 1 (a,1,b,1) -> (a,b,1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15dfeac1-e57a-4923-9aa7-7fcf2bc2fc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [5, 6, 1, 1],\n",
       "        [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones_like(x)\n",
    "z = torch.cat([x,y], dim=0)\n",
    "z.view(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6b20f16e-5dc6-48b5-bbfb-efcc3bd219e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.permute(dims=(-2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "24dbc9e4-67be-4096-9d78-3b9902912b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "tensor([[3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "\n",
      "tensor([[-1,  0,  1],\n",
      "        [ 2,  3,  4]])\n",
      "\n",
      "tensor([[ 2,  4,  6],\n",
      "        [ 8, 10, 12]])\n",
      "\n",
      "tensor([[0.5000, 1.0000, 1.5000],\n",
      "        [2.0000, 2.5000, 3.0000]])\n",
      "\n",
      "tensor([[0, 1, 1],\n",
      "        [2, 2, 3]])\n",
      "\n",
      "tensor([[0, 1, 1],\n",
      "        [0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    x,\n",
    "    x+2,\n",
    "    x-2,\n",
    "    x*2,\n",
    "    x/2,\n",
    "    x//2,\n",
    "    (x//2)%2,\n",
    "    sep='\\n\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d5a3c-39f8-411b-afe0-0faa058d3a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec29ded2-70fe-4203-9f08-f34f47cce75b",
   "metadata": {},
   "source": [
    "\n",
    "## Algebra\n",
    "```\n",
    "ret = A.mm(B)       # matrix multiplication\n",
    "ret = A.mv(x)       # matrix-vector multiplication\n",
    "x = x.t()           # matrix transpose\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4a805736-7801-432a-9485-22bb2246378c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 32,  6,  6],\n",
       "        [32, 77, 15, 15]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mm(z.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1d9dea58-7b44-44ae-9b85-c09f9b47d357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 32,  6,  6])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.mv(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667bb3a-93d1-476b-a260-9c6339e85845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdc53499-7e2d-4912-9ebd-885a0f3f7e7f",
   "metadata": {},
   "source": [
    "\n",
    "## GPU usage\n",
    "```\n",
    "torch.cuda.is_available                                     # check for cuda\n",
    "x = x.cuda()                                                # move x's data from\n",
    "                                                            # CPU to GPU and return new object\n",
    "\n",
    "x = x.cpu()                                                 # move x's data from GPU to CPU\n",
    "                                                            # and return new object\n",
    "\n",
    "if not args.disable_cuda and torch.cuda.is_available():     # device agnostic code\n",
    "    args.device = torch.device('cuda')                      # and modularity\n",
    "else:                                                       #\n",
    "    args.device = torch.device('cpu')                       #\n",
    "\n",
    "net.to(device)                                              # recursively convert their\n",
    "                                                            # parameters and buffers to\n",
    "                                                            # device specific tensors\n",
    "\n",
    "x = x.to(device)                                            # copy your tensors to a device\n",
    "                                                            # (gpu, cpu)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff578fa2-e576-4d9a-8ef1-75fc185dc86c",
   "metadata": {},
   "source": [
    "## Other functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "71543c6c-2fd9-4e42-a5aa-fc7cf2295c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4e1d7dc1-8f27-4ca4-9cd1-759627075928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ceil(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9ec8d609-eede-452a-a636-bbf62207e750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.floor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "548e9f62-d0e1-455e-86cd-c1533d5f37fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "adbbcfb9-47ef-4e17-ad2f-4864820a5142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1, -2, -3],\n",
       "        [-4, -5, -6]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.neg(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "67a2071a-b0c9-413d-ba69-05c305beaa86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 5]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(x, min=1, max=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a2808d07-b238-4637-accc-4c17f6e306a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[74., 51., 10.],\n",
       "        [94., 63., 38.],\n",
       "        [87., 34., 46.]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(size=(3,3), low=10, high=100, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "55f1c0e1-7b19-4864-8198-d6cc558132dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "tensor(21)\n",
      "\n",
      "tensor([5, 7, 9])\n",
      "\n",
      "tensor([ 6, 15])\n",
      "\n",
      "tensor(3)\n",
      "\n",
      "tensor(6)\n",
      "\n",
      "tensor(1)\n",
      "\n",
      "tensor(5)\n",
      "\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    x,\n",
    "    torch.sum(x),\n",
    "    torch.sum(x, dim=0),\n",
    "    torch.sum(x, dim=1),\n",
    "    # torch.mean(x, dim=1),\n",
    "    torch.median(x),\n",
    "    torch.max(x),\n",
    "    torch.min(x),\n",
    "    # torch.std(x),\n",
    "    # torch.var(x),\n",
    "    torch.argmax(x),\n",
    "    torch.argmin(x),\n",
    "    \n",
    "    sep = \"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2c4650bf-eea2-4379-a49d-4579e7b180c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 32,  6,  6],\n",
       "        [32, 77, 15, 15]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x, z.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "208ac74c-4276-4c05-af50-2406a50d639c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1, v2 = x[0], x[-1]\n",
    "torch.dot(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "25264f6e-d8c3-4be6-b390-6b77bbc65cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(x, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8b192efe-4c96-4619-b8c4-5484f3a5ee0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-52290.0156)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "a = torch.randint(size=(3,3), low=10, high=100, dtype=torch.float32)\n",
    "torch.det(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cb20a6f8-b4a6-475b-bbaf-4334216cc3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0578,  0.0716, -0.0282],\n",
       "        [ 0.0960, -0.0899,  0.0234],\n",
       "        [-0.0073, -0.0035,  0.0179]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.inverse(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "778c0e2f-04d2-41cb-a029-40df930e21ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "956cfe07-b9ae-49eb-ba05-18e9bcdc77e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.6931, 1.0986])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a94aaa40-3f9d-470e-8bc9-93bf8275c8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7183,  7.3891, 20.0855])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d47b34bf-f256-4188-8779-aff752713aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.4142, 1.7321])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d62669a3-47f0-4260-a64a-89e5b75ef543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311, 0.8808, 0.9526],\n",
       "        [0.9820, 0.9933, 0.9975],\n",
       "        [0.7311, 0.7311, 0.7311],\n",
       "        [0.7311, 0.7311, 0.7311]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7e2c2bcd-12be-4d3e-a42e-3f9a0f65ae7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0433, 0.0458, 0.0468],\n",
       "        [0.8700, 0.9205, 0.9405],\n",
       "        [0.0433, 0.0169, 0.0063],\n",
       "        [0.0433, 0.0169, 0.0063]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(z.float(), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a09283c9-18e1-4c5e-9834-7cfa66744636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5e5b1d02-c410-4255-8030-ba7e689cdff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a[:]\n",
    "id(a) == id(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
